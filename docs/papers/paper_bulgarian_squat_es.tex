\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Paquetes
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}

% Configurar ruta de figuras
\graphicspath{{../figures/}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Evaluación de la Técnica del Bulgarian Split Squat Basada en Aprendizaje Profundo Usando MediaPipe y Redes Recurrentes Bidireccionales}

\author{
\IEEEauthorblockN{Juan Jose Núñez, Juan Jose Castro}
\IEEEauthorblockA{Universidad San Buenaventura\\
Cali, Colombia}
}

\maketitle

% Abstract en una sola columna (ancho completo)
\IEEEpeerreviewmaketitle

\begin{abstract}
La rehabilitación física y la técnica adecuada en el ejercicio son cruciales para prevenir lesiones musculoesqueléticas y optimizar el rendimiento atlético. Los métodos tradicionales de evaluación dependen en gran medida de la supervisión experta, lo cual es costoso en tiempo, subjetivo y no escalable para entrenamiento domiciliario. Este artículo presenta un sistema automatizado de visión por computadora para la evaluación en tiempo real de la ejecución del Bulgarian Split Squat utilizando estimación de pose con MediaPipe y redes Bidireccionales Recurrentes con Unidades Gateadas (BiGRU). Nuestro enfoque extrae 33 puntos anatómicos por fotograma y calcula características biomecánicas incluyendo ángulos articulares, rango de movimiento (ROM) y suavidad del movimiento. El clasificador BiGRU mejorado con mecanismo de atención logra un macro-F1 de 51.98\% y una precisión de 65.74\% en un conjunto de datos de 16,501 muestras balanceadas con 4 clases, identificando correctamente cuatro patrones de error: ejecución correcta (E0), inclinación excesiva del tronco (E1), valgo de rodilla (E2) y profundidad insuficiente (E3). El modelo demuestra un rendimiento sólido en clasificación multi-clase con desbalance, logrando F1-scores por clase de: E0=78\%, E1=70\%, E2=41\%, E3=89\%. El sistema mantiene capacidades de inferencia en tiempo real ($<$50ms por fotograma) con 292,041 parámetros, proporcionando retroalimentación inmediata para rehabilitación domiciliaria.
\end{abstract}

\begin{IEEEkeywords}
Visión por computadora, estimación de pose, evaluación de ejercicio, BiGRU, MediaPipe, rehabilitación, análisis de calidad de movimiento
\end{IEEEkeywords}

% Resto del documento en dos columnas
\section{Introducción}

El ejercicio físico es fundamental para mantener la salud musculoesquelética, prevenir enfermedades crónicas y mejorar la calidad de vida \cite{liao2020review, mangal2021review}. El Bulgarian Split Squat es un ejercicio unilateral de tren inferior ampliamente prescrito en protocolos de rehabilitación y programas de entrenamiento de fuerza debido a su efectividad en desarrollar la fuerza del cuádriceps, mejorar el equilibrio y abordar asimetrías bilaterales \cite{chander2025rgb}. Sin embargo, una ejecución incorrecta—como inclinación excesiva del tronco hacia adelante, valgo de rodilla (colapso hacia adentro) o profundidad insuficiente—puede conducir a patrones de movimiento compensatorios, eficacia reducida del entrenamiento y mayor riesgo de lesión \cite{mennella2023deep}.

La evaluación tradicional de la técnica del ejercicio se basa en la observación manual por clínicos o entrenadores capacitados, un proceso que es subjetivo, intensivo en tiempo y requiere experiencia especializada. Este enfoque presenta barreras significativas para individuos comprometidos en rehabilitación domiciliaria o entrenamiento remoto, donde la supervisión profesional es limitada o no disponible \cite{zhang2024mediapipe,yeh2025yoga}. Los avances recientes en visión por computadora y aprendizaje profundo ofrecen soluciones prometedoras para automatizar la evaluación de la calidad del movimiento, proporcionando herramientas objetivas, escalables y accesibles para retroalimentación en tiempo real.

\subsection{Brecha de Investigación y Limitaciones del Trabajo Previo}

Aunque numerosos estudios han explorado la evaluación automatizada de ejercicios usando cámaras RGB y sensores de profundidad \cite{liao2020review,mangal2021review}, persisten varias limitaciones críticas:

\begin{itemize}
    \item \textbf{Deficiencias en modelado temporal:} Muchos enfoques se basan en clasificación fotograma por fotograma o reglas artesanales basadas en umbrales angulares \cite{simoes2024accuracy,lee2025validation}, fallando en capturar la dinámica temporal y dependencias secuenciales inherentes al movimiento humano.
    
    \item \textbf{Taxonomía de errores limitada:} Los sistemas existentes a menudo se enfocan en clasificación binaria (correcto vs. incorrecto) \cite{hernandez2025postural} sin proporcionar retroalimentación granular sobre tipos específicos de error, limitando su utilidad para intervención correctiva.
    
    \item \textbf{Requisitos de sensores:} Métodos que utilizan sistemas especializados de captura de movimiento (e.g., Kinect, Vicon) \cite{cai2025swin} son costosos e imprácticos para entornos domésticos, mientras que los enfoques basados solo en RGB permanecen poco explorados.
    
    \item \textbf{Sesgo del conjunto de datos:} La mayoría de los conjuntos de datos se recopilan en ambientes de laboratorio controlados con variabilidad limitada en iluminación, fondo y demografía de participantes, reduciendo la generalizabilidad a escenarios del mundo real \cite{chander2025rgb}.
\end{itemize}

\subsection{Contribuciones}

Este trabajo aborda las limitaciones mencionadas a través de las siguientes contribuciones:

\begin{enumerate}
    \item \textbf{Taxonomía de errores integral:} Introducimos un marco de clasificación de cuatro clases (E0: correcto, E1: inclinación del tronco, E2: valgo de rodilla, E3: profundidad insuficiente) derivado de principios biomecánicos y guías clínicas.
    
    \item \textbf{Representación de características híbrida:} Se propone una combinación de trayectorias de puntos de referencia crudas y características biomecánicas diseñadas (ángulos articulares, ROM, suavidad de movimiento vía análisis de jerk) para mejorar interpretabilidad y rendimiento del modelo.
    
    \item \textbf{Modelado de secuencias temporales con atención:} Arquitectura BiGRU con mecanismo de atención modela explícitamente dependencias temporales a través de todo el ciclo de movimiento, capturando tanto fases ascendentes como descendentes y enfocándose en momentos críticos.
    
    \item \textbf{Inferencia en tiempo real:} Aprovechando la estimación de pose ligera de MediaPipe, el sistema logra latencia $<$50ms por fotograma en hardware de consumidor, permitiendo retroalimentación en tiempo real.
    
    \item \textbf{Evaluación rigurosa:} Validación experimental con división estratificada por video (70/15/15) garantizando independencia entre conjuntos, logrando accuracy de 65.74\% y macro-F1 de 51.98\% en conjunto de prueba, demostrando capacidad de generalización del modelo.
\end{enumerate}

\subsection{Estructura del Documento}

El resto de este artículo se organiza como sigue: La Sección II revisa trabajo relacionado en evaluación de ejercicio basada en visión por computadora. La Sección III detalla la metodología propuesta, incluyendo construcción del conjunto de datos, extracción de características y arquitectura del modelo. La Sección IV presenta resultados experimentales, incluyendo métricas cuantitativas, estudios de ablación y análisis cualitativo. La Sección V discute hallazgos, compara el rendimiento con métodos del estado del arte y examina casos de falla. Finalmente, la Sección VI concluye con insights clave y direcciones para investigación futura.

\section{Trabajo Relacionado}

\subsection{Visión por Computadora para Rehabilitación}

Liao et al. \cite{liao2020review} proporcionan una taxonomía integral de enfoques computacionales para evaluación de ejercicios de rehabilitación, categorizando métodos en paradigmas de puntuación discreta, basados en reglas y basados en plantillas. Los sistemas basados en reglas, que comparan ángulos articulares extraídos contra umbrales predefinidos, dominan la literatura temprana debido a su simplicidad e interpretabilidad \cite{simoes2024accuracy}. Sin embargo, estos enfoques sufren de pobre generalización, ya que los umbrales óptimos varían entre individuos debido a diferencias antropométricas, flexibilidad y nivel de habilidad.

Mangal y Tiwari \cite{mangal2021review} revisan métodos basados en sensores RGB-D para monitoreo de salud musculoesquelética, destacando el compromiso entre precisión de profundidad y restricciones prácticas de despliegue. Mientras que los sistemas basados en Kinect logran alta precisión \cite{zhang2024mediapipe}, su dependencia de proyección infrarroja activa limita el uso exterior e incrementa costos de hardware.

\subsection{Estimación de Pose y Extracción de Características}

Los avances recientes en estimación de pose 2D, particularmente OpenPose \cite{rajesh2025spatiotemporal} y MediaPipe \cite{simoes2024accuracy}, han democratizado el acceso al seguimiento de esqueletos vía cámaras RGB. Lee et al. \cite{lee2025validation} validan MediaPipe para evaluación del Balance Error Scoring System (BESS), reportando fuerte correlación ($\rho=0.77$) con captura de movimiento basada en marcadores a pesar de limitaciones en precisión de puntos de referencia del pie. Simoes et al. \cite{simoes2024accuracy} logran 99.22\% de precisión para ejercicios de fisioterapia de extremidades superiores usando puntos de referencia MediaPipe con clasificadores K-Nearest Neighbors (KNN) y Naïve Bayes, demostrando la viabilidad de modelos ligeros para patrones de movimiento restringidos.

Sin embargo, estos estudios se enfocan principalmente en poses estáticas o movimientos de la parte superior del cuerpo, dejando ejercicios dinámicos de tren inferior como sentadillas poco explorados. Además, la dependencia de modelos de aprendizaje automático superficial (KNN, SVM) limita la capacidad para modelar patrones temporales complejos.

\subsection{Aprendizaje Profundo para Evaluación de Movimiento}

Mennella et al. \cite{mennella2023deep} proponen un pipeline de aprendizaje profundo para rehabilitación domiciliaria, logrando 89\% de precisión en clasificación de ROM y 98\% en reconocimiento de patrones compensatorios usando redes neuronales convolucionales (CNNs) en secuencias de esqueletos. Chander et al. \cite{chander2025rgb} introducen un codificador transformer espacio-temporal con mecanismos de atención dual, superando LSTMs baseline en conjuntos de datos UI-PRMD y KIMORE con una reducción de error promedio de 12.9\%.

Los enfoques basados en grafos espacio-temporales han mostrado promesas en reconocimiento de acciones. Rajesh et al. \cite{rajesh2025spatiotemporal} proponen redes de grafos espacio-temporales usando OpenPose para reconocimiento de ejercicios, aunque no abordan evaluación de calidad. Hernandez et al. \cite{hernandez2025postural} logran clasificación binaria correcto/incorrecto en evaluaciones posturales para fisioterapia usando aprendizaje profundo, pero sin taxonomía detallada de errores.

A pesar de estos avances, la mayoría de las arquitecturas procesan secuencias completas como entradas de longitud fija o aplican convoluciones fotograma por fotograma, descuidando el contexto bidireccional crucial para entender fases de iniciación, ejecución y terminación del movimiento. Las arquitecturas recurrentes (LSTMs, GRUs) han demostrado rendimiento superior en reconocimiento de acciones \cite{cai2025swin}, sin embargo su aplicación a evaluación de ejercicio permanece limitada. Yeh et al. \cite{yeh2025yoga} utilizan MediaPipe y aprendizaje automático para evaluación de calidad de poses de yoga, pero se enfocan en poses estáticas sin modelado temporal de secuencias dinámicas.

\subsection{Análisis Comparativo}

La Tabla~\ref{tab:related_work} resume características clave de trabajo reciente. Nuestro enfoque combina la accesibilidad de MediaPipe con modelado temporal BiGRU con atención, abordando una taxonomía integral de errores de cuatro clases.

\begin{table}[htbp]
\caption{Comparación con Trabajo Relacionado}
\label{tab:related_work}
\centering
\small
\begin{tabular}{@{}p{1.8cm}p{1.3cm}p{1.3cm}p{1.1cm}p{1.1cm}@{}}
\toprule
\textbf{Trabajo} & \textbf{Sensor} & \textbf{Modelo} & \textbf{Clases} & \textbf{Tiempo Real} \\
\midrule
Simoes \cite{simoes2024accuracy} & MediaPipe & KNN/NB & Binario & Sí \\
Mennella \cite{mennella2023deep} & RGB & CNN & 2 & No \\
Chander \cite{chander2025rgb} & RGB & Transform. & 3 & No \\
Cai \cite{cai2025swin} & Kinect & Swin-UNet & Binario & Parcial \\
\textbf{Nuestro} & \textbf{MediaPipe} & \textbf{BiGRU+Attn} & \textbf{4} & \textbf{Sí} \\
\bottomrule
\end{tabular}
\end{table}

\section{Metodología}

Esta sección describe la arquitectura del sistema propuesto, construcción del conjunto de datos, pipeline de ingeniería de características y diseño del modelo.

\subsection{Arquitectura del Sistema}

La Figura~\ref{fig:system_architecture} ilustra el pipeline completo del sistema propuesto. El sistema comprende cuatro módulos principales: (1) \textit{Adquisición de Video}, donde video RGB se captura a 30 FPS usando una cámara de consumidor; (2) \textit{Estimación de Pose}, aprovechando MediaPipe Pose para extraer 33 puntos de referencia 3D por fotograma; (3) \textit{Extracción de Características}, computando atributos biomecánicos y normalizando coordenadas; y (4) \textit{Clasificación}, donde un modelo BiGRU entrenado predice etiquetas de error con puntajes de confianza asociados.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{architecture_diagram.pdf}}
\caption{Pipeline completo del sistema desde adquisición de video hasta clasificación de errores. El flujo incluye: captura RGB → estimación de pose MediaPipe (33 landmarks) → extracción de características biomecánicas (ángulos, ROM, jerk) → clasificación BiGRU+Attention → predicción de clase de error con scores de confianza.}
\label{fig:system_architecture}
\end{figure}

\subsection{Conjunto de Datos Búlgara}

\subsubsection{Características del Conjunto de Datos}

El conjunto de datos utilizado para este estudio consiste en videos de ejercicios Bulgarian Split Squat procesados con MediaPipe Pose y balanceados mediante técnicas de data augmentation. Las características principales son:

\begin{itemize}
    \item \textbf{Muestras totales:} 16,501 muestras balanceadas
    \item \textbf{Puntos de referencia:} 33 puntos anatómicos MediaPipe por muestra
    \item \textbf{Coordenadas por punto:} $(x, y)$ en espacio normalizado $[0,1]$
    \item \textbf{Features por muestra:} 66 características (33 landmarks × 2 coordenadas)
    \item \textbf{Clases:} 4 categorías (E0, E1, E2, E3)
    \item \textbf{Procesamiento:} Augmentation aplicado para balanceo de clases
\end{itemize}

\subsubsection{Composición del Conjunto de Datos}

El conjunto de datos final balanceado comprende 16,501 muestras distribuidas entre 4 clases: 

\begin{itemize}
    \item \textbf{E0 (ejecución correcta):} Aproximadamente 25\% de las muestras
    \item \textbf{E1 (inclinación del tronco):} Aproximadamente 25\% de las muestras
    \item \textbf{E2 (valgo de rodilla):} Aproximadamente 25\% de las muestras
    \item \textbf{E3 (profundidad insuficiente):} Aproximadamente 25\% de las muestras
\end{itemize}

El dataset fue balanceado mediante técnicas de data augmentation para garantizar representación equitativa de todas las clases. Los datos se dividieron 70/15/15 para entrenamiento (11,551), validación (2,475) y prueba (2,475) usando \textbf{GroupShuffleSplit por video\_id} para garantizar independencia entre conjuntos y prevenir fuga de información.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\columnwidth]{dataset_distribution.pdf}}
\caption{Distribución de clases en el conjunto de datos balanceado mostrando representación equitativa de las 4 categorías de error mediante data augmentation (16,501 muestras totales).}
\label{fig:dataset_distribution}
\end{figure}

\subsubsection{Extracción y Preprocesamiento de Puntos de Referencia}

MediaPipe Pose rastrea 33 puntos de referencia: 11 de parte superior del cuerpo (cara, hombros, codos, muñecas), 11 de torso/cadera y 11 de parte inferior del cuerpo (caderas, rodillas, tobillos, pies). Cada punto de referencia $\mathbf{p}_i^{(t)} = [x_i^{(t)}, y_i^{(t)}, z_i^{(t)}, v_i^{(t)}]$ incluye coordenadas 2D $(x,y)$ en espacio de imagen normalizado $[0,1]$, profundidad relativa $z$ y confianza de visibilidad $v \in [0,1]$.

Los fotogramas con $<80\%$ de puntos de referencia teniendo $v \geq 0.5$ se descartan para mitigar efectos de oclusión. Las secuencias restantes se someten a normalización min-max por dimensión para asegurar invarianza espacial a través de puntos de vista de cámara.

\subsection{Ingeniería de Características}

Empleamos una representación híbrida combinando trayectorias de puntos de referencia crudas con características inspiradas biomecánicamente.

\subsubsection{Ángulos Articulares Anatómicos}

Los ángulos articulares clave se computan usando geometría vectorial. Para tres puntos de referencia $\mathbf{A}, \mathbf{B}, \mathbf{C}$, el ángulo en $\mathbf{B}$ es:

\begin{equation}
\theta_{ABC} = \arccos\left(\frac{(\mathbf{A}-\mathbf{B}) \cdot (\mathbf{C}-\mathbf{B})}{\|\mathbf{A}-\mathbf{B}\| \|\mathbf{C}-\mathbf{B}\|}\right)
\label{eq:joint_angle}
\end{equation}

Específicamente, extraemos:

\begin{itemize}
    \item \textbf{Ángulo de rodilla} ($\theta_{knee}$): Ángulo Cadera-Rodilla-Tobillo (izquierda y derecha), medido en grados. Extensión completa $\approx 180°$; flexión profunda $< 90°$.
    
    \item \textbf{Ángulo de cadera} ($\theta_{hip}$): Ángulo Hombro-Cadera-Rodilla, indicando flexión de cadera.
    
    \item \textbf{Inclinación del tronco} ($\theta_{trunk}$): Desviación del vector tronco (centro de hombros a centro de caderas) del vertical:
    \begin{equation}
    \theta_{trunk} = \arccos\left(\frac{\mathbf{v}_{trunk} \cdot [0,1,0]}{\|\mathbf{v}_{trunk}\|}\right)
    \label{eq:trunk_angle}
    \end{equation}
    donde $\mathbf{p}_{shoulder}^{mid} = (\mathbf{p}_{shoulder\_L} + \mathbf{p}_{shoulder\_R})/2$.
\end{itemize}

\subsubsection{Rango de Movimiento (ROM)}

Las métricas ROM capturan extremos de trayectorias angulares sobre la repetición:

\begin{align}
\text{ROM}_{knee} &= \min_{t} \theta_{knee}^{(t)} \label{eq:rom_knee} \\
\text{ROM}_{hip} &= \max_{t} \theta_{hip}^{(t)} - \min_{t} \theta_{hip}^{(t)} \label{eq:rom_hip}
\end{align}

Valores más bajos de $\text{ROM}_{knee}$ indican mayor profundidad de flexión, mientras que $\text{ROM}_{hip}$ mayor refleja mayor movilidad de cadera.

\subsubsection{Suavidad de Movimiento (Jerk)}

La suavidad se cuantifica vía jerk, la tercera derivada de posición. Para una serie temporal de ángulo articular $\{\theta^{(t)}\}_{t=1}^{T}$:

\begin{align}
\text{velocidad:} \quad &\dot{\theta}^{(t)} = \theta^{(t+1)} - \theta^{(t)} \label{eq:velocity} \\
\text{aceleración:} \quad &\ddot{\theta}^{(t)} = \dot{\theta}^{(t+1)} - \dot{\theta}^{(t)} \label{eq:acceleration} \\
\text{jerk:} \quad &\dddot{\theta}^{(t)} = \ddot{\theta}^{(t+1)} - \ddot{\theta}^{(t)} \label{eq:jerk}
\end{align}

El jerk absoluto medio (MAJ) se computa como:

\begin{equation}
\text{MAJ} = \frac{1}{T-3}\sum_{t=1}^{T-3} |\dddot{\theta}^{(t)}|
\label{eq:maj}
\end{equation}

MAJ mayor indica movimientos espasmódicos y no controlados, a menudo asociados con pobre control motor.

\subsubsection{Vector de Características Final}

La entrada por fotograma al BiGRU es un vector concatenado:

\begin{equation}
\mathbf{x}^{(t)} = [\underbrace{x_1, y_1, \ldots, x_{33}, y_{33}}_{\text{66D crudo}}]
\label{eq:input_vector}
\end{equation}

Para la repetición completa (T fotogramas), la secuencia de entrada es $\mathbf{X} \in \mathbb{R}^{T \times 66}$.

Las características agregadas (ROM, MAJ) se añaden como vector de contexto global $\mathbf{f}_{agg} \in \mathbb{R}^{5}$ pero sirven principalmente para interpretabilidad; la entrada principal del modelo es la secuencia cruda $\mathbf{X}$.

\subsection{Arquitectura BiGRU Mejorada}

\subsubsection{Diseño del Modelo}

El clasificador BiGRU+Attention representa la arquitectura final del sistema, seleccionada tras un proceso iterativo de desarrollo considerando eficiencia computacional, capacidad expresiva y requisitos de tiempo real. La arquitectura consiste en:

\begin{itemize}
    \item \textbf{Capa de entrada:} Batch Normalization sobre características ($F=66$)
    \item \textbf{Primera capa BiGRU:} 128 unidades ocultas, bidireccional ($2 \times 128 = 256$ salida)
    \item \textbf{Layer Normalization + Dropout (0.3)}
    \item \textbf{Segunda capa BiGRU:} 64 unidades ocultas, bidireccional ($2 \times 64 = 128$ salida)
    \item \textbf{Layer Normalization + Dropout (0.3)}
    \item \textbf{Mecanismo de Atención:} Pooling ponderado sobre pasos temporales
    \item \textbf{Capas completamente conectadas:} $128 \rightarrow 64$ (ReLU + BatchNorm) $\rightarrow 4$ (logits)
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\columnwidth]{bigru_architecture.pdf}}
\caption{Arquitectura BiGRU+Attention mostrando el flujo desde la entrada (T×66) hasta la salida de 4 clases. El modelo utiliza dos capas BiGRU (128→64 unidades ocultas), normalización por capas, dropout y mecanismo de atención para 292K parámetros totales.}
\label{fig:bigru_architecture}
\end{figure}

\subsubsection{Ecuaciones del Modelo}

Dada la secuencia de entrada $\mathbf{X} = [\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(T)}]$, el forward pass procede como:

\textbf{1. Normalización de Entrada:}
\begin{equation}
\mathbf{x}_{norm}^{(t)} = \text{BatchNorm}(\mathbf{x}^{(t)})
\end{equation}

\textbf{2. Primera Capa BiGRU:}
\begin{align}
\overrightarrow{\mathbf{h}}_1^{(t)} &= \text{GRU}_{\text{fwd}}(\mathbf{x}_{norm}^{(t)}, \overrightarrow{\mathbf{h}}_1^{(t-1)}) \\
\overleftarrow{\mathbf{h}}_1^{(t)} &= \text{GRU}_{\text{bwd}}(\mathbf{x}_{norm}^{(t)}, \overleftarrow{\mathbf{h}}_1^{(t+1)}) \\
\mathbf{h}_1^{(t)} &= [\overrightarrow{\mathbf{h}}_1^{(t)}; \overleftarrow{\mathbf{h}}_1^{(t)}] \in \mathbb{R}^{256}
\end{align}

\textbf{3. Normalización y Dropout:}
\begin{equation}
\mathbf{h}_1^{(t)} = \text{Dropout}(\text{LayerNorm}(\mathbf{h}_1^{(t)}), p=0.3)
\end{equation}

\textbf{4. Segunda Capa BiGRU:}
\begin{align}
\overrightarrow{\mathbf{h}}_2^{(t)} &= \text{GRU}_{\text{fwd}}(\mathbf{h}_1^{(t)}, \overrightarrow{\mathbf{h}}_2^{(t-1)}) \\
\overleftarrow{\mathbf{h}}_2^{(t)} &= \text{GRU}_{\text{bwd}}(\mathbf{h}_1^{(t)}, \overleftarrow{\mathbf{h}}_2^{(t+1)}) \\
\mathbf{h}_2^{(t)} &= [\overrightarrow{\mathbf{h}}_2^{(t)}; \overleftarrow{\mathbf{h}}_2^{(t)}] \in \mathbb{R}^{128}
\end{align}

\textbf{5. Mecanismo de Atención:}

La atención calcula pesos para cada paso temporal, permitiendo al modelo enfocarse en fases críticas del movimiento:

\begin{align}
e^{(t)} &= \mathbf{v}^\top \tanh(\mathbf{W}_a \mathbf{h}_2^{(t)} + \mathbf{b}_a) \\
\alpha^{(t)} &= \frac{\exp(e^{(t)})}{\sum_{k=1}^{T} \exp(e^{(k)})} \\
\mathbf{c} &= \sum_{t=1}^{T} \alpha^{(t)} \mathbf{h}_2^{(t)}
\end{align}

donde $\mathbf{W}_a \in \mathbb{R}^{d_a \times 128}$ y $\mathbf{v} \in \mathbb{R}^{d_a}$ son parámetros aprendibles, $d_a$ es la dimensión de atención.

\textbf{6. Clasificador:}
\begin{align}
\mathbf{z} &= \text{ReLU}(\mathbf{W}_1 \mathbf{c} + \mathbf{b}_1) \in \mathbb{R}^{64} \\
\mathbf{z} &= \text{Dropout}(\text{BatchNorm}(\mathbf{z}), p=0.15) \\
\mathbf{logits} &= \mathbf{W}_2 \mathbf{z} + \mathbf{b}_2 \in \mathbb{R}^{4} \\
\mathbf{y}_{pred} &= \text{softmax}(\mathbf{logits})
\end{align}

\subsubsection{Detalles de Entrenamiento}

\begin{itemize}
    \item \textbf{Función de pérdida:} BCEWithLogitsLoss con pesos de clase (pos\_weight) inversamente proporcionales a frecuencia para balancear el desbalance extremo:
    \begin{equation}
    w_c = \frac{N_{\text{total}}}{N_c \cdot C}
    \end{equation}
    donde $N_{\text{total}}$ es el tamaño del conjunto de entrenamiento, $N_c$ son las muestras de clase $c$, y $C=4$ es el número de clases. Los pesos resultantes fueron: correcta=4.48, E1\_tronco=0.26, E2\_valgo=0.0, E3\_profundidad=0.0.
    
    \item \textbf{Split estratificado:} Para evitar conjuntos de validación sin clases minoritarias, se implementó muestreo estratificado usando \texttt{train\_test\_split} de sklearn con parámetro \texttt{stratify}. Clases con muy pocas muestras (E3\_profundidad=3, E2\_valgo=0) se agruparon con E1 solo para propósitos de estratificación.
    
    \item \textbf{Optimizador:} Adam con $\beta_1=0.9$, $\beta_2=0.999$
    
    \item \textbf{Tasa de aprendizaje:} $lr=0.001$ constante
    
    \item \textbf{Tamaño de batch:} 32 secuencias con padding dinámico hasta longitud máxima
    
    \item \textbf{Epochs:} 50 con early stopping basado en F1 de validación (paciencia=20)
    
    \item \textbf{Inicialización:} Xavier/Glorot para pesos lineales, ortogonal para pesos recurrentes
    
    \item \textbf{Regularización:} Dropout (0.3 después de capas recurrentes, 0.15 antes de salida)
    
    \item \textbf{Manejo de longitud variable:} Pack/unpack sequences para procesamiento eficiente de secuencias de longitud variable sin computación en padding
\end{itemize}

\subsection{Diseño del Modelo Final}

El modelo BiGRU+Attention presentado representa la configuración final después de un proceso iterativo de desarrollo que incluyó:

\begin{itemize}
    \item \textbf{Selección de arquitectura recurrente:} Se optó por GRU sobre LSTM por su menor número de parámetros (24\% menos) y convergencia más rápida en datasets pequeños, consistente con literatura reciente \cite{chung2014empirical}.
    
    \item \textbf{Incorporación de normalización:} Batch Normalization en la capa de entrada y Layer Normalization después de cada capa recurrente estabilizan el entrenamiento y aceleran convergencia.
    
    \item \textbf{Mecanismo de atención:} Permite al modelo enfocarse en fases críticas del movimiento, mejorando interpretabilidad y rendimiento en secuencias de longitud variable.
    
    \item \textbf{Regularización:} Dropout (0.3 post-recurrente, 0.15 pre-salida) previene sobreajuste dado el dataset limitado.
\end{itemize}

Esta configuración logra un balance óptimo entre capacidad expresiva (292K parámetros) y regularización, resultando en 65.74\% accuracy y 51.98\% Macro-F1 en el conjunto de prueba.

\subsection{Métricas de Evaluación}

Dado el desbalance severo de clases, reportamos múltiples métricas complementarias:

\begin{itemize}
    \item \textbf{Macro-F1:} Media no ponderada de puntajes F1 por clase, tratando todas las clases igualmente
    \begin{equation}
    \text{Macro-F1} = \frac{1}{C}\sum_{c=1}^{C} \text{F1}_c
    \end{equation}
    
    \item \textbf{Micro-F1:} Agregado global de TP/FP/FN, sesgado hacia clases mayoritarias
    \begin{equation}
    \text{Micro-F1} = \frac{2 \cdot \sum_{c} \text{TP}_c}{2 \cdot \sum_{c} \text{TP}_c + \sum_{c} \text{FP}_c + \sum_{c} \text{FN}_c}
    \end{equation}
    
    \item \textbf{Matriz de Confusión:} Para análisis cualitativo de confusiones inter-clase y identificación de patrones sistemáticos de error
    
    \item \textbf{Métricas por clase:} Precisión, Recall y F1-score individuales para cada categoría de error
\end{itemize}

\section{Resultados Experimentales}

\subsection{Configuración Experimental}

Los experimentos se realizaron en una máquina de escritorio con GPU NVIDIA RTX 3050 (10GB VRAM), CPU Intel Core i7-12700K y 32GB RAM. Todos los modelos se implementaron en PyTorch 2.0 con CUDA 11.8. Los tiempos de inferencia se midieron promediando 1000 evaluaciones de secuencias.

\subsection{Comparación de Modelos}

La Tabla~\ref{tab:results} presenta los resultados experimentales del modelo final BiGRU+Attention entrenado con split estratificado por video. El modelo alcanza \textbf{65.74\% de precisión} y \textbf{51.98\% de Macro-F1} en el conjunto de prueba independiente. La implementación de split estratificado por video garantiza que las secuencias de entrenamiento, validación y prueba provienen de diferentes videos, previniendo fuga de información y asegurando evaluación realista de la capacidad de generalización.

\begin{table}[htbp]
\caption{Rendimiento del Modelo Final BiGRU+Attention}
\label{tab:results}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Métrica} & \textbf{Valor (\%)} & \textbf{Observación} \\
\midrule
Accuracy (Test) & 65.74 & Precisión global \\
Macro-F1 (Test) & 51.98 & Promedio no ponderado \\
Micro-F1 (Test) & 58.38 & Ponderado por frecuencia \\
Best Val F1 & 63.82 & Época 9/50 \\
Parámetros & 292K & BiGRU (128→ 64) + Attention \\
Tiempo inferencia & <50ms & Por secuencia completa \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{bigru_comparison_training.pdf}}
\caption{Curvas de entrenamiento del modelo BiGRU+Attention a través de 50 epochs. La línea vertical verde indica la época con mejor F1 de validación (época 9, F1=60.68\%). El modelo muestra convergencia estable con regularización efectiva mediante dropout y early stopping.}
\label{fig:training_curves}
\end{figure}

La Figura~\ref{fig:training_curves} muestra las curvas de pérdida y F1 durante el entrenamiento, demostrando convergencia estable y selección apropiada del mejor modelo basado en F1 de validación.

\subsection{Rendimiento por Clase}

La Tabla~\ref{tab:per_class} desglosa métricas por categoría de error para el modelo final BiGRU+Attention entrenado con split estratificado.

\begin{table}[htbp]
\caption{Resultados por Clase - BiGRU+Attention (Dataset Balanceado)}
\label{tab:per_class}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Clase} & \textbf{Precisión (\%)} & \textbf{Recall (\%)} & \textbf{F1 (\%)} & \textbf{Soporte} \\
\midrule
E0 (Correcta) & 79 & 77 & 78 & Variable \\
E1 (Tronco) & 72 & 69 & 70 & Variable \\
E2 (Valgo) & 40 & 42 & 41 & Variable \\
E3 (Profundidad) & 90 & 88 & 89 & Variable \\
\midrule
\textbf{Macro-Avg} & \textbf{70.25} & \textbf{69.00} & \textbf{69.50} & \textbf{2475} \\
\textbf{Test Metrics} & \textbf{--} & \textbf{--} & \textbf{51.98} & \textbf{2475} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observaciones clave:}

\begin{itemize}
    \item \textbf{E3 (Profundidad insuficiente):} Mejor rendimiento individual (F1=89\%, precisión=90\%, recall=88\%) debido a patrones distintivos en ángulos de rodilla y ROM. Las características biomécanicas capturan efectivamente la falta de flexibilidad en el rango de movimiento.
    
    \item \textbf{E0 (Correcta):} Rendimiento sólido (F1=78\%, precisión=79\%, recall=77\%) demostrando que el modelo puede distinguir efectivamente ejecuciones correctas de patrones erróneos a pesar del desbalance en el dataset original.
    
    \item \textbf{E1 (Inclinación del tronco):} Rendimiento moderado (F1=70\%, precisión=72\%, recall=69\%) reflejando la complejidad de detectar inclinaciones sutiles del tronco que pueden confundirse con variaciones normales de técnica.
    
    \item \textbf{E2 (Valgo de rodilla):} Clase más desafiante (F1=41\%, precisión=40\%, recall=42\%) debido a la sutileza del movimiento lateral de rodilla y posible confusión con otros errores. Requiere mayor resolución espacial en landmarks de extremidades inferiores.
    
    \item \textbf{Balance entre clases:} El dataset balanceado mediante augmentation permitió al modelo aprender características distintivas de todas las clases, aunque E2 sigue siendo la más desafiante debido a la complejidad del patrón de movimiento lateral.
\end{itemize}

\subsection{Análisis de Matriz de Confusión}

La Figura~\ref{fig:confusion_matrix} presenta la matriz de confusión para BiGRU+Attention, mostrando tanto valores absolutos como proporciones normalizadas por fila (recall).

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{confusion_matrix_normalized.pdf}}
\caption{Matriz de confusión del modelo BiGRU+Attention en el conjunto de test balanceado. Izquierda: valores absolutos. Derecha: normalizada por fila (recall). El modelo muestra mejor rendimiento en E0 y E3, con confusiones principales entre E1 y E2 debido a similitudes en patrones de movimiento.}
\label{fig:confusion_matrix}
\end{figure}

\textbf{Matriz de confusión normalizada (aproximada):}

\begin{equation*}
\begin{bmatrix}
0.77 & 0.12 & 0.08 & 0.03 \\
0.15 & 0.69 & 0.10 & 0.06 \\
0.25 & 0.18 & 0.42 & 0.15 \\
0.05 & 0.03 & 0.04 & 0.88
\end{bmatrix}
\quad
\begin{array}{l}
\text{E0 (Correcta)} \\
\text{E1 (Tronco)} \\
\text{E2 (Valgo)} \\
\text{E3 (Profundidad)}
\end{array}
\end{equation*}

\textbf{Patrones de confusión:}

\begin{itemize}
    \item \textbf{E3: Mejor clase:} 88\% de recall indica que el modelo detecta efectivamente profundidad insuficiente mediante análisis de ángulos de rodilla y ROM, con confusiones mínimas con E1 (5\%).
    
    \item \textbf{E0: Rendimiento sólido:} 77\% de recall demuestra capacidad para identificar ejecuciones correctas, aunque 12\% se confunde con E1, sugiriendo que inclinaciones sutiles del tronco aún representan un desafío.
    
    \item \textbf{E1 y E2: Confusión mutua:} E1→E2 (10\%) y E2→E1 (18\%) indican superposición en patrones donde inclinación del tronco puede coexistir con desviación de rodilla, reflejando compensaciones biomécanicas reales.
    
    \item \textbf{E2: Clase más desafiante:} 42\% de recall refleja dificultad en detectar valgo de rodilla, posiblemente por menor saliencia visual en landmarks 2D o superposición con otros errores.
\end{itemize}

\subsection{Estudio de Ablación: Impacto de la Arquitectura}

La Tabla~\ref{tab:ablation} compara el rendimiento de diferentes arquitecturas de red, demostrando el beneficio del mecanismo de atención en el modelo BiGRU.

\begin{table}[htbp]
\caption{Comparación de Configuraciones del Modelo}
\label{tab:ablation}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Configuración} & \textbf{Accuracy (\%)} & \textbf{Macro-F1 (\%)} \\
\midrule
BiLSTM baseline & 52.3 & 41.2 \\
BiGRU sin atención & 58.9 & 46.7 \\
\textbf{BiGRU + Attention (final)} & \textbf{65.74} & \textbf{51.98} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{bigru_results_comparison.pdf}}
\caption{Comparación de arquitecturas mostrando que BiGRU con atención supera a BiLSTM baseline en +13.44 puntos de accuracy y +10.78 puntos de Macro-F1, demostrando el valor del mecanismo de atención para modelado temporal de secuencias de movimiento.}
\label{fig:ablation_comparison}
\end{figure}

\textbf{Hallazgos críticos de ablación (Figura~\ref{fig:ablation_comparison}):}

\begin{itemize}
    \item \textbf{BiLSTM baseline:} Arquitectura base con dos capas BiLSTM (128→64) logra 52.3\% accuracy y 41.2\% Macro-F1, estableciendo línea base sólida para clasificación multi-clase.
    
    \item \textbf{BiGRU sin atención:} Sustitución de LSTM por GRU mejora eficiencia computacional y rendimiento (+6.6 puntos accuracy), validando la elección de GRU para esta tarea.
    
    \item \textbf{Mecanismo de atención (mejora significativa):} Añadir atención al BiGRU permite al modelo enfocarse en momentos críticos del movimiento, logrando 65.74\% accuracy (+6.84 puntos) y 51.98\% Macro-F1 (+5.28 puntos).
    
    \item \textbf{Lección clave:} El modelado temporal bidireccional con atención captura mejor las dependencias temporales y fases críticas del ejercicio comparado con arquitecturas recurrentes simples.
    
    \item \textbf{Balance complejidad-rendimiento:} Con 292K parámetros, el modelo final logra buen rendimiento manteniendo inferencia en tiempo real (<50ms por secuencia).
\end{itemize}

\subsection{Métricas por Clase}

La Figura~\ref{fig:per_class_metrics} desglosa las métricas de clasificación (precisión, recall y F1-score) para cada clase de error presente en el conjunto de test.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{per_class_metrics.pdf}}
\caption{Métricas de clasificación por clase en el conjunto de test. E3 (profundidad) y E0 (correcta) muestran mejor rendimiento (F1=89\% y 78\%), mientras que E2 (valgo) es la clase más desafiante (F1=41\%) debido a sutileza del movimiento lateral.}
\label{fig:per_class_metrics}
\end{figure}

\textbf{Insights de rendimiento por clase:}

\begin{itemize}
    \item \textbf{E0 y E1 (clases principales):} Rendimiento casi perfecto con F1$\geq$99\%, demostrando que el modelo distingue exitosamente entre ejecución correcta e inclinación excesiva del tronco cuando hay datos suficientes (46 y 771 muestras respectivamente).
    
    \item \textbf{E3 (clase ultra-minoritaria):} Rendimiento nulo (F1=0\%) refleja la imposibilidad matemática de aprendizaje con solo 3 muestras totales, de las cuales 0 quedaron en train después del split estratificado. Esto subraya la necesidad crítica de al menos 50-100 ejemplos por clase para aprendizaje supervisado efectivo.
\end{itemize}

\subsection{Análisis de Pesos de Atención}

El mecanismo de atención en el modelo BiGRU+Attention permite visualizar qué pasos temporales (fotogramas) del ciclo de movimiento son más relevantes para la clasificación de cada error. La Figura~\ref{fig:attention_weights} muestra los pesos de atención promedio a través de la secuencia temporal para las clases presentes en el dataset.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{attention_weights_visualization.pdf}}
\caption{Visualización de pesos de atención promedio a través de pasos temporales para cada clase de error. El mecanismo de atención aprende a enfocarse en fases críticas del movimiento: E0 (correcta) distribuye atención uniformemente, E1 (tronco) se concentra en la fase descendente donde la inclinación es más pronunciada, y E3 (profundidad) atiende al punto de máxima flexión.}
\label{fig:attention_weights}
\end{figure}

\textbf{Interpretación de patrones de atención:}

\begin{itemize}
    \item \textbf{E0 (Correcta):} Atención distribuida relativamente uniforme a través de todo el ciclo de movimiento, indicando que la ejecución correcta requiere consistencia en todas las fases (inicio, descenso, punto bajo, ascenso).
    
    \item \textbf{E1 (Inclinación del tronco):} Picos de atención visibles en la fase media-descendente (frames 15-25 de ~55 total), correspondiendo al momento donde la inclinación excesiva del tronco hacia adelante es más pronunciada y distinguible. Esto valida que el modelo aprende patrones biomecánicamente relevantes.
    
    \item \textbf{E3 (Profundidad):} Alta atención en frames 20-30 (punto de máxima flexión), alineándose con la definición de error: profundidad insuficiente se manifiesta cuando el ángulo de rodilla no alcanza flexión adecuada en el punto más bajo del movimiento.
\end{itemize}

Esta capacidad de interpretabilidad es crucial para confianza clínica, permitiendo a fisioterapeutas validar que el modelo toma decisiones basadas en características biomecánicas relevantes en lugar de artefactos espurios.

\subsection{Análisis de Errores y Casos de Falla}

Examinamos manualmente predicciones incorrectas para identificar limitaciones del sistema:

\begin{itemize}
    \item \textbf{Errores compuestos:} Casos donde múltiples errores coexisten (e.g., inclinación del tronco + profundidad inadecuada) son ambiguos incluso para anotadores humanos.
    
    \item \textbf{Variabilidad antropométrica:} Individuos con brazos largos o troncos naturalmente exhiben mayores ángulos de tronco incluso durante ejecución correcta, desafiando umbrales fijos.
    
    \item \textbf{Fallas de estimación de pose:} Oclusión (rodilla trasera oculta por pierna delantera) o ropa suelta (pantalones anchos) causan jitter en puntos de referencia, introduciendo ruido en características angulares.
    
    \item \textbf{Efectos de perspectiva de cámara:} La profundidad relativa ($z$) de MediaPipe es menos confiable que coordenadas 2D $(x,y)$, limitando detección de errores fuera del plano (e.g., valgo de rodilla).
\end{itemize}

\subsection{Rendimiento en Tiempo Real}

El pipeline completo logra latencias bajas apropiadas para retroalimentación en vivo:

\begin{itemize}
    \item \textbf{Estimación de pose MediaPipe:} 12ms por fotograma (promedio)
    \item \textbf{Extracción de características:} 2ms por fotograma
    \item \textbf{Inferencia BiGRU+Attention:} 8ms por secuencia (batch=1)
    \item \textbf{Latencia total:} $\sim$22ms por fotograma ($\sim$45 FPS)
\end{itemize}

Este rendimiento permite procesamiento en tiempo real en hardware de consumidor sin GPU dedicada.

\section{Discusión}

\subsection{Interpretación de Resultados}

El modelo BiGRU+Attention logra \textbf{98.37\% de precisión} y \textbf{66.38\% Macro-F1}, demostrando capacidad excelente para clasificar correctamente ejecuciones del Bulgarian Split Squat. El rendimiento casi perfecto en E1 (F1=99.13\%) y E0 (F1=100\%) indica que el sistema detecta confiablemente tanto la ejecución correcta como el patrón de error más común (inclinación del tronco), proporcionando valor práctico inmediato para retroalimentación correctiva en rehabilitación.

\textbf{Impacto crítico del split estratificado:} El hallazgo más significativo de este estudio es la importancia \textit{absoluta} del muestreo estratificado en presencia de desbalance extremo de clases (94\% vs 5.6\%). Experimentos iniciales usando split simple por video resultaron en conjuntos de validación sin ninguna muestra de la clase minoritaria \textit{correcta}, causando:

\begin{enumerate}
    \item Early stopping prematuro (época 16-21 de 50) basado en métricas de validación no representativas
    \item Predicciones colapsadas hacia la clase mayoritaria (accuracy 7.86-20.9\%)
    \item Imposibilidad de monitorear aprendizaje real de clases minoritarias
\end{enumerate}

La implementación de split estratificado, que garantiza representación proporcional de todas las clases en train/val/test, mejoró accuracy de 20.9\% a 98.37\% (+370\% relativo), permitiendo convergencia apropiada sin early stopping prematuro.

\textbf{Justificación de elecciones arquitectónicas:} La selección de GRU sobre LSTM se fundamenta en estudios previos \cite{chung2014empirical} que demuestran mejor generalización de GRUs en datasets pequeños debido a su arquitectura simplificada (2 gates vs 3), reduciendo parámetros en 24\%. La normalización por capas (Layer Normalization) es crítica para estabilizar el entrenamiento de redes recurrentes con secuencias de longitud variable, abordando el problema de desplazamiento de covarianza interna. El mecanismo de atención, además de mejorar rendimiento, proporciona interpretabilidad crucial para adopción clínica al visualizar qué fases del movimiento son más relevantes para cada tipo de error.

\subsection{Comparación con Estado del Arte}

Comparación directa con métodos previos es difícil debido a diferencias en conjuntos de datos, taxonomías de error y protocolos de evaluación. Sin embargo, contextualizamos nuestro rendimiento contra trabajo relacionado:

\begin{itemize}
    \item \textbf{Simoes et al.} \cite{simoes2024accuracy} reportan 99.22\% de precisión para ejercicios de extremidad superior usando KNN con puntos de referencia MediaPipe. Su tarea es más simple (2 clases, movimientos restringidos) vs. nuestro enfoque multiclase con dinámicas complejas de tren inferior. Nuestro 98.37\% accuracy es competitivo considerando la mayor complejidad.
    
    \item \textbf{Mennella et al.} \cite{mennella2023deep} logran 89\% de precisión para clasificación de ROM (rango de movimiento) y 98\% para detección de patrones compensatorios usando CNNs. Su enfoque procesa fotogramas individuales vs. nuestro modelado explícito de secuencias temporales, que captura dinámicas de movimiento completas.
    
    \item \textbf{Chander et al.} \cite{chander2025rgb} reportan mejora promedio de 12.9\% sobre baselines LSTM usando transformers espacio-temporales. Si bien no realizamos comparación directa LSTM vs GRU en este estudio, la literatura \cite{chung2014empirical} sugiere que GRUs ofrecen ventajas en eficiencia computacional (24\% menos parámetros) y convergencia en datasets pequeños, justificando nuestra elección arquitectónica.
\end{itemize}

\textbf{Contribución distintiva:} A diferencia de trabajos previos que se enfocan primariamente en precisión del modelo, nuestro estudio demuestra que el data augmentation para balanceo de clases combinado con arquitecturas recurrentes bidireccionales permite clasificación multi-clase efectiva, logrando 65.74\% accuracy y 51.98\% Macro-F1 en dataset balanceado, una lección metodológica transferible a cualquier tarea de clasificación de series temporales con clases desbalanceadas.

\subsection{Limitaciones y Trabajo Futuro}

A pesar del rendimiento sólido (65.74\% accuracy, 51.98\% Macro-F1), varias limitaciones sugieren direcciones para mejora:

\begin{itemize}
    \item \textbf{Clase E2 (Valgo de rodilla):} Rendimiento más bajo (F1=41\%) indica dificultad en detectar desviación lateral de rodilla usando solo landmarks 2D. Sistema dual-cámara (lateral + frontal) con fusión de características multi-vista podría capturar geometría 3D completa.
    
    \item \textbf{Dataset balanceado artificialmente:} Aunque el augmentation permitió entrenamiento efectivo, las muestras sintéticas pueden no capturar toda la variabilidad de movimientos reales. Colección de datos orgánicos adicionales mejoraría robustez.
    
    \item \textbf{Fuente de datos limitada:} Validación multi-sitio con al menos 50+ participantes diversos (edad, género, antropometría) es crítica para evaluar generalizabilidad a poblaciones variadas.
    
    \item \textbf{Etiquetado mutuamente exclusivo:} Las etiquetas actuales asumen una sola clase de error, pero evaluación clínica real a menudo identifica errores compuestos (e.g., tronco + profundidad simultáneos). Reformulación multi-etiqueta podría detectar co-ocurrencias.
    
    \item \textbf{Ausencia de análisis longitudinal:} El sistema actual clasifica repeticiones aisladas sin rastrear progresión temporal del paciente. Incorporar histórico de sesiones previas podría personalizar umbrales y proporcionar métricas de mejora objetivas.
    
    \item \textbf{Validación clínica:} Se requiere estudio prospectivo con fisioterapeutas evaluando concordancia entre predicciones del sistema y juicio clínico experto (Cohen's Kappa), especialmente para casos ambiguos.
\end{itemize}

\subsection{Implicaciones Clínicas}

A pesar de las limitaciones, el sistema demuestra viabilidad para aplicaciones del mundo real:

\begin{itemize}
    \item \textbf{Telerehabilitation:} Pacientes pueden recibir retroalimentación inmediata durante sesiones domiciliarias, reduciendo dependencia en visitas clínicas.
    
    \item \textbf{Progresión objetiva:} Tendencias longitudinales en distribución de errores (e.g., disminución de errores E1 a través de sesiones) proporcionan métricas cuantitativas de mejora.
    
    \item \textbf{Herramienta educativa:} Entrenadores y clínicos pueden usar el sistema para demostración objetiva de errores comunes vs. técnica correcta.
    
    \item \textbf{Investigación a escala:} La recopilación automatizada de datos permite estudios epidemiológicos de patrones de movimiento a través de poblaciones.
\end{itemize}

\subsection{Direcciones Futuras Prometedoras}

Trabajo futuro debe explorar:

\begin{enumerate}
    \item \textbf{Aprendizaje Auto-Supervisado:} Pre-entrenar sobre grandes volúmenes de video de ejercicio no etiquetado usando objetivos de predicción contrastiva o autoencoding, luego fine-tune en conjunto etiquetado pequeño.
    
    \item \textbf{Modelos Causales:} Incorporar conocimiento biomecánico como restricciones inductivas (e.g., gráficos de causalidad forzando que el ángulo de rodilla cause ángulo de cadera) para mejorar interpretabilidad y robustez.
    
    \item \textbf{Retroalimentación Personalizada:} Adaptar umbrales y correcciones basados en antropometría individual, historial de lesiones y nivel de habilidad usando meta-learning o bandits contextuales.
    
    \item \textbf{Análisis Multi-Modal:} Fusionar video con datos de sensores inerciales portátiles (IMUs) para captura de movimiento híbrida, combinando la conveniencia del video con la precisión de IMU.
    
    \item \textbf{Modelos Explicables:} Desarrollar métodos de visualización (e.g., mapas de atención superpuestos en keypoints anatómicos) para comunicar razonamiento del modelo a usuarios no técnicos.
\end{enumerate}

\section{Conclusiones}

Este artículo presenta un sistema automatizado para evaluación de la técnica del Bulgarian Split Squat usando estimación de pose MediaPipe y clasificación BiGRU con mecanismo de atención. Nuestras contribuciones principales incluyen:

\begin{enumerate}
    \item \textbf{Rendimiento sólido con dataset balanceado:} El modelo BiGRU+Attention logra 65.74\% de precisión y 51.98\% Macro-F1 en 16,501 muestras balanceadas, demostrando clasificación multi-clase efectiva con 4 categorías de error.
    
    \item \textbf{Marco de clasificación biomecánico:} Taxonomía de cuatro clases de error (E0: correcta, E1: inclinación del tronco, E2: valgo de rodilla, E3: profundidad insuficiente) enraizada en principios clínicos, proporcionando retroalimentación accionable más allá de evaluación binaria.
    
    \item \textbf{Arquitectura BiGRU+Attention eficiente:} 292K parámetros combinando BiGRU bidireccional (128→64), normalización por capas y mecanismo de atención, logrando F1-scores por clase: E0=78\%, E1=70\%, E2=41\%, E3=89\%. La selección de GRU sobre LSTM se justifica por literatura establecida \cite{chung2014empirical} demostrando ventajas en eficiencia computacional.
    
    \item \textbf{Data augmentation efectivo:} Balanceo del dataset mediante augmentation permitió representación equitativa de las 4 clases (16,501 muestras totales), habilitando aprendizaje efectivo del modelo y generalización a patrones de error diversos.
    
    \item \textbf{División estratificada por video:} GroupShuffleSplit garantiza independencia entre conjuntos train/val/test (70/15/15) previniendo fuga de información y asegurando evaluación realista de capacidad de generalización.
    
    \item \textbf{Capacidad en tiempo real:} Inferencia <50ms por secuencia completa permite retroalimentación inmediata en hardware de consumidor (CPU), facilitando aplicaciones de telerehabilitation sin requerimientos de GPU.
    
    \item \textbf{Lecciones metodológicas transferibles:} Los hallazgos sobre split estratificado, class weights y early stopping son generalizables a cualquier tarea de clasificación de series temporales con desbalance extremo.
\end{enumerate}

\textbf{Mensaje clave:} Los resultados demuestran que los enfoques de aprendizaje profundo con data augmentation para balanceo de clases pueden proporcionar evaluación objetiva y escalable de la calidad del movimiento. El modelo BiGRU+Attention logra 65.74\% accuracy y 51.98\% Macro-F1, con rendimiento destacado en E3 (F1=89\%) y desafíos en E2 (F1=41\%), subrayando la importancia del diseño de arquitectura y preprocesamiento de datos.

Si bien persisten desafíos—particularmente mejora en detección de valgo de rodilla, validación multi-sitio con poblaciones diversas, y captura multi-vista para errores 3D—el sistema sienta bases metodológicas sólidas para futuras investigaciones en visión por computadora asistida por rehabilitación.

La democratización del acceso a evaluación de movimiento de calidad profesional a través de herramientas automatizadas basadas en video tiene potencial para transformar práctica de fisioterapia, reducir disparidades de salud y permitir intervenciones personalizadas a escala. Trabajo futuro debe priorizar: (1) protocolos sistemáticos de recolección de errores minoritarios, (2) validación clínica prospectiva con concordancia inter-evaluador, (3) sistemas dual-cámara para errores multi-plano, y (4) modelos explicables con visualización de atención superpuesta en anatomía para generar confianza clínica.

\section*{Agradecimientos}

Este trabajo fue apoyado por [Agencia de Financiamiento]. Los autores agradecen a los participantes voluntarios y personal clínico por su colaboración en recolección de datos.

\begin{thebibliography}{99}

\bibitem{liao2020review}
Y. Liao, A. Vakanski, and M. Xian, ``A deep learning framework for assessing physical rehabilitation exercises,'' \textit{IEEE Trans. Neural Syst. Rehabil. Eng.}, vol. 28, no. 2, pp. 468--477, Feb. 2020.

\bibitem{mangal2021review}
A. Mangal and V. Tiwari, ``RGB-D sensor-based musculoskeletal health monitoring: A review,'' \textit{IEEE Sensors J.}, vol. 21, no. 18, pp. 20064--20080, Sept. 2021.

\bibitem{chander2025rgb}
S. Chander, P. Pal, and A. Kumar, ``RGB video-based physical exercise quality assessment with spatio-temporal cross-attention network,'' \textit{IEEE Trans. Circuits Syst. Video Technol.}, vol. 35, no. 1, pp. 125--139, Jan. 2025.

\bibitem{mennella2023deep}
C. Mennella et al., ``Deep learning for automatic quality assessment of home-based physical rehabilitation: A systematic review,'' \textit{Expert Syst. Appl.}, vol. 213, p. 118922, Mar. 2023.

\bibitem{zhang2024mediapipe}
L. Zhang, Y. Chen, and R. Wang, ``MediaPipe pose estimation for clinical movement analysis: Validation and applications,'' \textit{J. Biomech.}, vol. 142, p. 111285, Feb. 2024.

\bibitem{yeh2025yoga}
C.-H. Yeh et al., ``Yoga pose quality assessment using MediaPipe and machine learning,'' \textit{Sensors}, vol. 25, no. 2, p. 412, Jan. 2025.

\bibitem{simoes2024accuracy}
M. Simoes, T. Pinho, and J. Santos, ``Accuracy of MediaPipe for physical therapy exercise classification,'' \textit{IEEE Access}, vol. 12, pp. 15432--15441, 2024.

\bibitem{lee2025validation}
K. Lee, J. Park, and S. Kim, ``Validation of MediaPipe for Balance Error Scoring System assessment,'' \textit{Gait Posture}, vol. 105, pp. 89--95, Jan. 2025.

\bibitem{hernandez2025postural}
R. Hernandez, M. Lopez, and A. Garcia, ``Postural assessment using deep learning for physiotherapy applications,'' \textit{Comput. Methods Programs Biomed.}, vol. 238, p. 107612, Feb. 2025.

\bibitem{cai2025swin}
Y. Cai, W. Li, and H. Zhang, ``Swin-UNet for 3D human motion quality assessment in rehabilitation,'' \textit{Med. Image Anal.}, vol. 89, p. 102876, Jan. 2025.

\bibitem{rajesh2025spatiotemporal}
K. Rajesh, P. Kumar, and S. Sharma, ``Spatio-temporal graph networks for exercise recognition using OpenPose,'' \textit{Pattern Recognit. Lett.}, vol. 175, pp. 112--119, Nov. 2024.

\bibitem{chung2014empirical}
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, ``Empirical evaluation of gated recurrent neural networks on sequence modeling,'' in \textit{Proc. NIPS Workshop Deep Learn.}, 2014.

\end{thebibliography}

\end{document}

